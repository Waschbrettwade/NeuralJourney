{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this very first chapter, I will build my very first Neural Network, a \"Linear Associative Network\".\n",
    "# I am following \"Artificial Intelligence Engines: A Tutorial Introduction to the Mathematics of Deep \n",
    "# Learning\" As described in the Readme, at least at this point, I will not go through the mathematics in \n",
    "# this GitHub, but rather find out and show how to implement it. I will comment the code and doing so, I \n",
    "# will provide links that will explain the mathematics or other code.\n",
    "\n",
    "# That being said, let's get started with our very first Neural Network!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import * # in order to create a random number between 0 and 1 using random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNN:\n",
    "    # If you are not familiar with classes (I am new to this either at this point),\n",
    "    # https://docs.python.org/3/tutorial/classes.html# might be helpful for you.\n",
    "    \n",
    "    def fit(x_train, y_train, learning_rate, tol): \n",
    "    \n",
    "        w = random()  # Create a random weight between 0 and 1 \n",
    "        learning = True \n",
    "        # If learning = True, the while loop will run. If learning = False, it will interrupt\n",
    "        \n",
    "        k = 0 # Just initiating a number so that we can have a track of the number of iterations\n",
    "    \n",
    "        while learning:\n",
    "    \n",
    "            y_pred = w * x_train # Make a first \"prediction\"\n",
    "            delta = y_pred - y_train # Calculate the difference between the label and the prediction\n",
    "            w_diff = - learning_rate * delta * x_train\n",
    "            # At this very point, we will be learning using gradient descent. If you are not \n",
    "            # familiar with gradient descent, you might want to read this\n",
    "            # https://www.cantorsparadise.com/gradient-descent-for-machine-learning-explained-35b3e9dcc0eb\n",
    "            # and if you are have or have gotten some hang of it, try understanding a more formal \n",
    "            # explanation like this:\n",
    "            # https://en.wikipedia.org/wiki/Gradient_descent#Description\n",
    "            \n",
    "            w = w + w_diff # Change our weight\n",
    "        \n",
    "            if np.abs(delta) < tol: learning = False\n",
    "            # If the difference between our prediction and our label is greater than tol,\n",
    "            # keep learning. Else: stop by setting learning to False.\n",
    "            \n",
    "            k += 1 # Iteration count\n",
    "        \n",
    "            # Printing y_pred and w will only be useful for the examples below.\n",
    "            print(y_pred, w)\n",
    "            \n",
    "        # Save the weight inside of the class so that our next function, predict, can use the weight\n",
    "        simpleNN.weights = w\n",
    "        \n",
    "        return w, y_pred, delta, k\n",
    "    \n",
    "    def predict(x_pred):\n",
    "        return simpleNN.weights*x_pred # Just make the linear prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.498545176416043 1.0495635529248128\n",
      "2.0991271058496257 1.2297381317548877\n",
      "2.4594762635097753 1.3378428790529326\n",
      "2.6756857581058653 1.4027057274317596\n",
      "2.805411454863519 1.4416234364590557\n",
      "2.8832468729181113 1.4649740618754334\n",
      "2.929948123750867 1.47898443712526\n",
      "2.95796887425052 1.487390662275156\n",
      "2.974781324550312 1.4924343973650935\n",
      "2.984868794730187 1.495460638419056\n",
      "2.990921276838112 1.4972763830514337\n",
      "2.9945527661028675 1.4983658298308602\n",
      "2.9967316596617204 1.499019497898516\n",
      "2.998038995797032 1.4994116987391097\n",
      "2.9988233974782195 1.4996470192434659\n",
      "2.9992940384869318 1.4997882115460794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.4997882115460794, 2.9992940384869318, -0.0007059615130682317, 16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpleNN.fit(2,3, 0.1, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4997882115460794"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpleNN.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4997882115460794"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpleNN.predict(1) # Perfect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.999576423092159"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpleNN.predict(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving weights in a self method doesn't seem to make much sense, because self would need to be after the \n",
    "# fitting. But if we do not fit, self wouldn't be able to make an assignment like\n",
    "# self.weights = weights. Because of this circumstance, we will at least for now continue to make \n",
    "# assignments like that: ***** class.value = value ***** as in line 37."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: You will need to be very aware of your learning rate. The gradient depends hugely on the rate\n",
    "# and will explode or even oscillate between two values if not set correctly. I will give you some\n",
    "# examples as comments, because the output will be infinite and I haven't found out how to \n",
    "# collapse an output yet. You can copy the code and run it and see what is happening here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Oscillating:\n",
    "# simpleNN.fit(2,3, 0.5, 0.001)\n",
    "\n",
    "##### This learning rate of 10e-7 seems to be somewhat of a sweet spot.\n",
    "# simpleNN.fit(2021,17.3, 0.0000001, 0.001)\n",
    "\n",
    "##### However, if we decrease the learning rate further, we will get to the correct result, too, but it \n",
    "##### will take longer.\n",
    "# simpleNN.fit(2021,17.3, 0.00000001, 0.001)\n",
    "\n",
    "##### On the other hand, if we increase the learning rate, the gradient will explode isntantly and five us NaNs.\n",
    "##### Exploding example 1\n",
    "#simpleNN.fit(2021,17.3, 0.000001, 0.001)\n",
    "\n",
    "##### Exploding example 2\n",
    "# simpleNN.fit(2021,17.3, 0.01, 0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
